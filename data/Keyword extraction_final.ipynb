{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        keyword_list = []\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            keyword_list.append(key)\n",
    "            #print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                return keyword_list\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rhinovirus', 'detection', 'Qiagen', 'transmission', 'UV', 'irradiation', 'method', 'RT', 'aerosols', 'air', 'studies', 'PCR']\n"
     ]
    }
   ],
   "source": [
    "# tr4w = TextRank4Keyword()\n",
    "# text = '''BACKGROUND: Rhinovirus, the most common cause of upper respiratory tract infections, has been implicated in asthma exacerbations and possibly asthma deaths. Although the method of transmission of rhinoviruses is disputed, several studies have demonstrated that aerosol transmission is a likely method of transmission among adults. As a first step in studies of possible airborne rhinovirus transmission, we developed methods to detect aerosolized rhinovirus by extending existing technology for detecting infectious agents in nasal specimens. METHODS: We aerosolized rhinovirus in a small aerosol chamber. Experiments were conducted with decreasing concentrations of rhinovirus. To determine the effect of UV irradiation on detection of rhinoviral aerosols, we also conducted experiments in which we exposed aerosols to a UV dose of 684 mJ/m(2). Aerosols were collected on Teflon filters and rhinovirus recovered in Qiagen AVL buffer using the Qiagen QIAamp Viral RNA Kit (Qiagen Corp., Valencia, California) followed by semi-nested RT-PCR and detection by gel electrophoresis. RESULTS: We obtained positive results from filter samples that had collected at least 1.3 TCID(50 )of aerosolized rhinovirus. Ultraviolet irradiation of airborne virus at doses much greater than those used in upper-room UV germicidal irradiation applications did not inhibit subsequent detection with the RT-PCR assay. CONCLUSION: The air sampling and extraction methodology developed in this study should be applicable to the detection of rhinovirus and other airborne viruses in the indoor air of offices and schools. This method, however, cannot distinguish UV inactivated virus from infectious viral particles.'''\n",
    "# tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "# snig_list = []\n",
    "# snig_list = tr4w.get_keywords(10)\n",
    "# print(snig_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('info_ret_textrank_new.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41549 entries, 0 to 41548\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    41549 non-null  int64 \n",
      " 1   title         41549 non-null  object\n",
      " 2   abstract      41549 non-null  object\n",
      " 3   publish_time  41549 non-null  object\n",
      " 4   authors       41549 non-null  object\n",
      " 5   url           41549 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41549 entries, 0 to 41548\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    41549 non-null  int64 \n",
      " 1   title         41549 non-null  object\n",
      " 2   abstract      41549 non-null  object\n",
      " 3   publish_time  41549 non-null  object\n",
      " 4   authors       41549 non-null  object\n",
      " 5   url           41549 non-null  object\n",
      " 6   text          41549 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = dataset['title'] + dataset['abstract']\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snigdhasingh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:126: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/Users/snigdhasingh/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:129: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "final_list = []\n",
    "temp_list = []\n",
    "tr4w = TextRank4Keyword()\n",
    "for i in range (0, 41549):\n",
    "    x = dataset[i,6]\n",
    "    tr4w.analyze(str(x), candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "    temp_list = tr4w.get_keywords(10)\n",
    "    if temp_list is None:\n",
    "        temp_list = tr4w.get_keywords(5)\n",
    "    final_list.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['muscle', 'eIF3f', 'MAFbx', 'initiation', 'hypertrophy', 'conditions', 'role', 'atrogin-1', 'atrophy', 'subunit', 'protein', 'translation']\n"
     ]
    }
   ],
   "source": [
    "print(final_list[41548])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "dataset = dataset.assign(keywords=final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41549 entries, 0 to 41548\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   0         41549 non-null  object\n",
      " 1   1         41549 non-null  object\n",
      " 2   2         41549 non-null  object\n",
      " 3   3         41549 non-null  object\n",
      " 4   4         41549 non-null  object\n",
      " 5   5         41549 non-null  object\n",
      " 6   6         41549 non-null  object\n",
      " 7   keywords  41461 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>BACKGROUND: Rhinovirus, the most common cause ...</td>\n",
       "      <td>2003-01-13</td>\n",
       "      <td>Myatt, Theodore A; Johnston, Sebastian L; Rudn...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "      <td>Airborne rhinovirus detection and effect of ul...</td>\n",
       "      <td>[rhinovirus, detection, Qiagen, transmission, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Discovering human history from stomach bacteria</td>\n",
       "      <td>Recent analyses of human pathogens have reveal...</td>\n",
       "      <td>2003-04-28</td>\n",
       "      <td>Disotell, Todd R</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "      <td>Discovering human history from stomach bacteri...</td>\n",
       "      <td>[pathogens, histories, Helicobacter, groups, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A new recruit for the army of the men of death</td>\n",
       "      <td>The army of the men of death, in John Bunyan's...</td>\n",
       "      <td>2003-06-27</td>\n",
       "      <td>Petsko, Gregory A</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...</td>\n",
       "      <td>A new recruit for the army of the men of death...</td>\n",
       "      <td>[recruit, men, army, phrase, face, Bunyan, fear]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>BACKGROUND: The human leukocyte antigen (HLA) ...</td>\n",
       "      <td>2003-09-12</td>\n",
       "      <td>Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "      <td>Association of HLA class I with severe acute r...</td>\n",
       "      <td>[SARS, HLA, group, coronavirus, P, Pc, risk, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>A double epidemic model for the SARS propagation</td>\n",
       "      <td>BACKGROUND: An epidemic of a Severe Acute Resp...</td>\n",
       "      <td>2003-09-10</td>\n",
       "      <td>Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...</td>\n",
       "      <td>A double epidemic model for the SARS propagati...</td>\n",
       "      <td>[SARS, epidemic, model, spread, epidemics, dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1  \\\n",
       "0  0  Airborne rhinovirus detection and effect of ul...   \n",
       "1  1    Discovering human history from stomach bacteria   \n",
       "2  2     A new recruit for the army of the men of death   \n",
       "3  3  Association of HLA class I with severe acute r...   \n",
       "4  4   A double epidemic model for the SARS propagation   \n",
       "\n",
       "                                                   2           3  \\\n",
       "0  BACKGROUND: Rhinovirus, the most common cause ...  2003-01-13   \n",
       "1  Recent analyses of human pathogens have reveal...  2003-04-28   \n",
       "2  The army of the men of death, in John Bunyan's...  2003-06-27   \n",
       "3  BACKGROUND: The human leukocyte antigen (HLA) ...  2003-09-12   \n",
       "4  BACKGROUND: An epidemic of a Severe Acute Resp...  2003-09-10   \n",
       "\n",
       "                                                   4  \\\n",
       "0  Myatt, Theodore A; Johnston, Sebastian L; Rudn...   \n",
       "1                                   Disotell, Todd R   \n",
       "2                                  Petsko, Gregory A   \n",
       "3  Lin, Marie; Tseng, Hsiang-Kuang; Trejaut, Jean...   \n",
       "4  Ng, Tuen Wai; Turinici, Gabriel; Danchin, Antoine   \n",
       "\n",
       "                                                   5  \\\n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...   \n",
       "1  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...   \n",
       "2  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1...   \n",
       "3  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...   \n",
       "4  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2...   \n",
       "\n",
       "                                                   6  \\\n",
       "0  Airborne rhinovirus detection and effect of ul...   \n",
       "1  Discovering human history from stomach bacteri...   \n",
       "2  A new recruit for the army of the men of death...   \n",
       "3  Association of HLA class I with severe acute r...   \n",
       "4  A double epidemic model for the SARS propagati...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [rhinovirus, detection, Qiagen, transmission, ...  \n",
       "1  [pathogens, histories, Helicobacter, groups, p...  \n",
       "2   [recruit, men, army, phrase, face, Bunyan, fear]  \n",
       "3  [SARS, HLA, group, coronavirus, P, Pc, risk, I...  \n",
       "4  [SARS, epidemic, model, spread, epidemics, dis...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
